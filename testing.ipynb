{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<button onclick=\"$('.input, .prompt, .output_stderr, .output_error').toggle();\">Toggle Code</button>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import display_html\n",
    "display_html(\"\"\"<button onclick=\"$('.input, .prompt, .output_stderr, .output_error').toggle();\">Toggle Code</button>\"\"\", raw=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# OpenStreetMap\n",
    "***Data Wrangling with mongoDB by NK Zhehua Zou***\n",
    "  \n",
    "Map Area: San Jose, CA, United States  \n",
    "https://mapzen.com/data/metro-extracts/metro/san-jose_california/  \n",
    "  \n",
    "***Table of Contents***\n",
    "1. Data Audit\n",
    "2. Problems Encountered in the Map  \n",
    "Abbreviated Street Names  \n",
    "Postal Codes  \n",
    "3. Data Overview  \n",
    "4. Additional Ideas  \n",
    "Contributor statistics and gamification suggestion  \n",
    "Additional data exploration using MongoDB  \n",
    "5. Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Data Audit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Load packages and libraries\n",
    "import sys\n",
    "sys.path.append(\"script/\")\n",
    "import xml.etree.cElementTree as ET\n",
    "import re\n",
    "\n",
    "### cleaning ###\n",
    "from collections import defaultdict\n",
    "import string\n",
    "\n",
    "### osm to json ###\n",
    "\n",
    "from pymongo import MongoClient\n",
    "import schem\n",
    "import os\n",
    "import codecs\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Load data\n",
    "# This data just a sample for code testing, I didn't change analysis from original data\n",
    "# Please read html file if you want to reviewed entire analysis.\n",
    "\n",
    "data = 'data/sample.osm'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tags\n",
    "Parse through the San Jose dataset with ElementTree and count the number of unique element types to get an overall understanding of the data by using count_tags function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# This function will takes 5~10 seconds. Be patient on on next step.\n",
    "# Parse through the data with ElementTree.\n",
    "def count_tags(data):\n",
    "    tags={}\n",
    "    for event, elem in ET.iterparse(data):\n",
    "        if elem.tag in tags:\n",
    "            tags[elem.tag]+=1\n",
    "        else:\n",
    "            tags[elem.tag]=1\n",
    "    return tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'bounds': 1,\n",
       " 'member': 115,\n",
       " 'nd': 21564,\n",
       " 'node': 13332,\n",
       " 'osm': 1,\n",
       " 'relation': 21,\n",
       " 'tag': 7128,\n",
       " 'way': 3721}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_tags(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Keys Type\n",
    "*** For the follinwg function: key_type & process_map. We check the \"k\" value for each. ***  \n",
    "\"lower\", for tags that contain only lowercase letters and are valid.  \n",
    "\"lower_colon\", for otherwise valid tags with a colon in their names.  \n",
    "\"problemchars\", for tags with problematic characters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Count of each of three tag categories in a dictionary with re.\n",
    "lower = re.compile(r'^([a-z]|_)*$')\n",
    "lower_colon = re.compile(r'^([a-z]|_)*:([a-z]|_)*$')\n",
    "# This regex represents invalid MongoDB characters for keys.\n",
    "problemchars = re.compile(r'[=\\+/&<>;\\'\"\\?%#$@\\,\\. \\t\\r\\n]')\n",
    "\n",
    "def key_type(element, keys):\n",
    "    if element.tag == 'tag':\n",
    "        if re.match(lower,element.get('k'))!=None:\n",
    "            keys['lower']+=1\n",
    "        elif re.match(lower_colon,element.get('k'))!=None:\n",
    "            keys['lower_colon']+=1\n",
    "        elif re.match(problemchars,element.get('k'))!=None:\n",
    "            keys['problemchars']+=1\n",
    "        else:\n",
    "            keys['other']+=1\n",
    "    return keys\n",
    "\n",
    "def process_key(data):\n",
    "    keys = {'lower': 0, 'lower_colon': 0, 'problemchars': 0, 'other': 0}\n",
    "    for _, element in ET.iterparse(data):\n",
    "        keys = key_type(element, keys)\n",
    "    return keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'lower': 6751, 'lower_colon': 363, 'other': 14, 'problemchars': 0}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "process_key(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "94 peoples invovlved in the map editing.\n"
     ]
    }
   ],
   "source": [
    "def process_people(data):\n",
    "    users = set()\n",
    "    for _, element in ET.iterparse(data):\n",
    "        for e in element:\n",
    "            if 'uid' in e.attrib:\n",
    "                users.add(e.attrib['uid'])\n",
    "    return users\n",
    "\n",
    "print str(len(process_people(data))) + ' peoples invovlved in the map editing.'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Problems Encountered in the Map\n",
    "After initially downloading a small sample size of the San Jose area and running it, I noticed three main problems with the data, which I will discuss in the following order:  \n",
    "1) Abbreviated street names ('Branham Ln')  \n",
    "2) Inconsistent postal codes ('CA950543', '95014-1899')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Abbreviated Street Names\n",
    "Once the data was imported to MongoDB, some basic querying revealed street name abbreviations. I updated all substrings in problematic address strings, such that 'Branham Ln' becomes 'Branham Lane'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "street_type_re = re.compile(r'\\b\\S+\\.?$', re.IGNORECASE)\n",
    "street_abbrev_re = re.compile(r'^([a-z]){1}\\.?(\\s)+', re.IGNORECASE)\n",
    "expected = ['Avenue', 'Boulevard', 'Commons', 'Court', 'Drive', 'Lane', 'Parkway', \n",
    "                         'Place', 'Road', 'Square', 'Street', 'Trail']\n",
    "mapping_street = {'Ave'  : 'Avenue',\n",
    "           'Blvd' : 'Boulevard',\n",
    "           'Dr'   : 'Drive',\n",
    "           'Ln'   : 'Lane',\n",
    "           'Pkwy' : 'Parkway',\n",
    "           'Rd'   : 'Road',\n",
    "           'Rd.'   : 'Road',\n",
    "           'St'   : 'Street',\n",
    "           'street' :'Street',\n",
    "           'Ct'   : 'Court',\n",
    "           'Cir'  : 'Circle',\n",
    "           'Cr'   : 'Court',\n",
    "           'ave'  : 'Avenue',\n",
    "           'Hwg'  : 'Highway',\n",
    "           'Hwy'  : 'Highway',\n",
    "           'Sq'   : 'Square'}\n",
    "\n",
    "mapping_abbrev = { 'W ': 'West ', 'S ': 'South ', 'N ': 'North ', 'E ': 'East ',\\\n",
    "                   'W. ': 'West ', 'S. ': 'South', 'N. ': 'North ', 'E. ': 'East '}\n",
    "\n",
    "def audit_street_type(street_types, street_name):\n",
    "    m = street_type_re.search(street_name)\n",
    "    if m:\n",
    "        street_type = m.group()\n",
    "        if street_type not in expected:\n",
    "            street_types[street_type].add(street_name)\n",
    "\n",
    "def is_street_name(elem):\n",
    "    return (elem.tag == 'tag') and (elem.attrib['k'] == 'addr:street')\n",
    "\n",
    "def audit(osmfile):\n",
    "    osm_file = open(osmfile, 'r')\n",
    "    street_types = defaultdict(set)\n",
    "    for event, elem in ET.iterparse(osm_file, events=('start',)):\n",
    "        if elem.tag == \"node\" or elem.tag == 'way':\n",
    "            for tag in elem.iter('tag'):\n",
    "                if is_street_name(tag):\n",
    "                    audit_street_type(street_types, tag.attrib['v'])\n",
    "    return street_types\n",
    "\t\n",
    "def update_street(name, mapping_street, mapping_abbrev):\n",
    "    m = street_type_re.search(name)\n",
    "    if m:\n",
    "        street_type = m.group()\n",
    "        if street_type in mapping_street:\n",
    "            name = re.sub(regex, mapping[street_type], name)\n",
    "    # Updating W , E, N, S to West, East, North, South; if they are at the begining of an address.\n",
    "    m_1 = street_abbrev_re.search(name)\n",
    "    if m_1:\n",
    "        street_abbrev = m_1.group()\n",
    "        if street_abbrev in mapping_abbrev.keys():\n",
    "            name = re.sub(street_abbrev, mapping_abbrev[street_abbrev], name)\n",
    "    # capitalizing first letter of all words in problematic address\n",
    "    name = string.capwords(name)\n",
    "    return name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1) The main problem we encountered in this dataset come from the street name abbreviation inconsistency. We build the regex matching the last element in the string, where usually the street type is based. Then we come up with a list of mapping that need not to be cleaned.  \n",
    "2) audit_street_type function search the input string for the regex. If there is a match and it is not within the 'expected' list, add the match as a key and add the string to the set.  \n",
    "3) is_street_name function looks at the attribute k if k='addre:street'.  \n",
    "4) audit functio will return the list that match previous two functions.  \n",
    "5) After that, we would do a pretty print the output of the audit. With the list of all the abbreviated street types we can understand and fill-up our 'mapping' dictionary as a preparatio to convert these street name into proper form. (list of 1)  \n",
    "6) update_name is the last step of the process, which take the old name and update them with a better name. (list of 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "San Jose => San Jose\n",
      "Arica => Arica\n",
      "Las Diamelas => Las Diamelas\n",
      "Maria Izaga => Maria Izaga\n",
      "Elias Aguirre => Elias Aguirre\n",
      "Avenida Luis Gonzales => Avenida Luis Gonzales\n",
      "Yahuar Huaca => Yahuar Huaca\n",
      "Simon Bolivar => Simon Bolivar\n",
      "Salaverry => Salaverry\n",
      "Av. Jose Balta => Av. Jose Balta\n",
      "Leoncio Prado => Leoncio Prado\n",
      "Conquista => Conquista\n",
      "Av. Pedro Ruiz => Av. Pedro Ruiz\n",
      "la Libertad => La Libertad\n",
      "Francisco Cabrera => Francisco Cabrera\n",
      "Husares de Junin => Husares De Junin\n",
      "Alfonso Ugarte => Alfonso Ugarte\n",
      "Paul Harris => Paul Harris\n",
      "Loreto => Loreto\n",
      "Manuel Seoane => Manuel Seoane\n",
      "Congreso => Congreso\n",
      "Juan Tomis Stack => Juan Tomis Stack\n",
      "Los Amautas => Los Amautas\n",
      "El Eden => El Eden\n",
      "Panamericana norte => Panamericana Norte\n",
      "Sáenz Peña => Sáenz Peña\n",
      "Elvira Garcia Y Garcia => Elvira Garcia Y Garcia\n",
      "Los Andes => Los Andes\n",
      "Avenida Oriente => Avenida Oriente\n",
      "Pasaje manuel seoane => Pasaje Manuel Seoane\n",
      "eduardo mesa => Eduardo Mesa\n",
      "Av. Angamos => Av. Angamos\n",
      "Zona Industrial => Zona Industrial\n",
      "Las Ñustas => Las Ñustas\n",
      "Pachacutec => Pachacutec\n",
      "Calle Tacna => Calle Tacna\n"
     ]
    }
   ],
   "source": [
    "for street_type, ways in audit(data).iteritems():\n",
    "    for name in ways:\n",
    "        better_name = update_street(name, mapping_street, mapping_abbrev)\n",
    "        print name, '=>', better_name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Postal Codes\n",
    "Postal code strings posed a different sort of problem, forcing a decision to strip all leading and trailing characters before and after the main 5-digit zip code. This effectually dropped all leading state characters (as in 'CA950543') and 4-digit zip code extensions following a hyphen ('95014-1899'). This 5-digit constriction benefits MongoDB aggregation calls on postal codes.  \n",
    "1) Although most of the zip code is correct, there're still a lot of zip code with incorrect 5 digit formats. We will process it like update street name. (list of 1)  \n",
    "2 )The output of the clean zip code is summarised below. There are the format of 5 digits. (list of 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def audit_zipcode(invalid_zipcodes, zipcode):\n",
    "    twoDigits = zipcode[0:2]\n",
    "    if not twoDigits.isdigit():\n",
    "        invalid_zipcodes[twoDigits].add(zipcode)\n",
    "    elif twoDigits != 95:\n",
    "        invalid_zipcodes[twoDigits].add(zipcode)\n",
    "        \n",
    "def is_zipcode(elem):\n",
    "    return (elem.tag == \"tag\") and (elem.attrib['k'] == 'addr:postcode')\n",
    "\n",
    "def audit_zip(osmfile):\n",
    "    osm_file = open(osmfile, 'r')\n",
    "    invalid_zipcodes = defaultdict(set)\n",
    "    for event, elem in ET.iterparse(osm_file, events=('start',)):\n",
    "        if elem.tag == 'node' or elem.tag == 'way':\n",
    "            for tag in elem.iter('tag'):\n",
    "                if is_zipcode(tag):\n",
    "                    audit_zipcode(invalid_zipcodes,tag.attrib['v'])\n",
    "    return invalid_zipcodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "140101 => None\n",
      "14820 => None\n",
      "074 => None\n"
     ]
    }
   ],
   "source": [
    "def update_postal(zipcode):\n",
    "    testNum = re.findall('[a-zA-Z]*', zipcode)\n",
    "    if testNum:\n",
    "        testNum = testNum[0]\n",
    "    testNum.strip()\n",
    "    if testNum == 'CA':\n",
    "        convertedZipcode = (re.findall(r'\\d+', zipcode))\n",
    "        if convertedZipcode:\n",
    "            return (re.findall(r'\\d+', zipcode))[0]\n",
    "    elif re.match(r'^95\\d+', zipcode):\n",
    "        return re.findall(r'\\d{5}', zipcode)[0]\n",
    "\n",
    "for street_type, ways in audit_zip(data).iteritems():\n",
    "    for name in ways:\n",
    "        better_name = update_postal(name)\n",
    "        print name, '=>', better_name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Data Overview\n",
    "This section contains basic statistics about the dataset and the MongoDB queries used to gather them.  \n",
    "  \n",
    "### Preparing for MongoDB by converting XML to JSON\n",
    "In order to transform the data from XML to JSON, we need to follow these rules:  \n",
    "1) Process only 2 types of top level tags: \"node\" and \"way\"  \n",
    "2) All attributes of \"node\" and \"way\" should be turned into regular key/value pairs, except:   attributes in the CREATED array should be added under a key \"created\", attributes for latitude and longitude should be added to a \"pos\" array, for use in geospacial indexing. Make sure the values inside \"pos\" array are floats and not strings.  \n",
    "3) If second level tag \"k\" value contains problematic characters, it should be ignored  \n",
    "4) If second level tag \"k\" value starts with \"addr:\", it should be added to a dictionary \"address\"  \n",
    "5) If second level tag \"k\" value does not start with \"addr:\", but contains \":\", you can process it same as any other tag.  \n",
    "6) If there is a second \":\" that separates the type/direction of a street, the tag should be ignored  \n",
    "After all the cleaning and data transformation are done, we would use last function process_map and convert the file from XML into JSON format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "client = MongoClient()\n",
    "db=client.project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "db.doc.drop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "lower = re.compile(r'^([a-z]|_)*$')\n",
    "lower_colon = re.compile(r'^([a-z]|_)*:([a-z]|_)*$')\n",
    "problemchars = re.compile(r'[=\\+/&<>;\\'\"\\?%#$@\\,\\. \\t\\r\\n]')\n",
    "address_regex = re.compile(r'^addr\\:')\n",
    "street_regex = re.compile(r'^street')\n",
    "\n",
    "# loading schema from schemaa file; The schemaa file is placed in the same directory that this notebook is placed;\n",
    "### I really don't know why have to use this for JSON ###\n",
    "SCHEMA = schem.schema\n",
    "\n",
    "CREATED = [ \"version\", \"changeset\", \"timestamp\", \"user\", \"uid\"]\n",
    "\n",
    "def shape_element(element):\n",
    "    for el_tag in element.iter('tag'):\n",
    "        key_tag = el_tag.attrib['k']\n",
    "        # for cleaning data from problematic characters\n",
    "        if problemchars.search(key_tag):\n",
    "            continue\n",
    "        # Fixing Street names\n",
    "        if is_street_name(el_tag):\n",
    "            el_tag.attrib['v'] = update_street(el_tag.attrib['v'], mapping_street, mapping_abbrev)\n",
    "        \n",
    "    node = {}\n",
    "    if element.tag == \"node\" or element.tag == \"way\" :\n",
    "        node['type'] = element.tag\n",
    "        # initialize empty address\n",
    "        address = {}\n",
    "        # parsing through attributes\n",
    "        for a in element.attrib:\n",
    "            if a in CREATED:\n",
    "                if 'created' not in node:\n",
    "                    node['created'] = {}\n",
    "                node['created'][a] = element.get(a)\n",
    "            elif a in ['lat', 'lon']:\n",
    "                continue\n",
    "            else:\n",
    "                node[a] = element.get(a)\n",
    "        # populate position\n",
    "        if 'lat' in element.attrib and 'lon' in element.attrib:\n",
    "            node['pos'] = [float(element.get('lat')), float(element.get('lon'))]\n",
    "\n",
    "        # parse second-level tags for nodes\n",
    "        for e in element:\n",
    "            # parse second-level tags for ways and populate `node_refs`\n",
    "            if e.tag == 'nd':\n",
    "                if 'node_refs' not in node:\n",
    "                    node['node_refs'] = []\n",
    "                if 'ref' in e.attrib:\n",
    "                    node['node_refs'].append(e.get('ref'))\n",
    "\n",
    "            # throw out not-tag elements and elements without `k` or `v`\n",
    "            if e.tag != 'tag' or 'k' not in e.attrib or 'v' not in e.attrib:\n",
    "                continue\n",
    "            key = e.get('k')\n",
    "            val = e.get('v')\n",
    "\n",
    "            # skip problematic characters\n",
    "            if problemchars.search(key):\n",
    "                continue\n",
    "\n",
    "            # parse address k-v pairs\n",
    "            elif address_regex.search(key):\n",
    "                key = key.replace('addr:', '')\n",
    "                address[key] = val\n",
    "\n",
    "            # catch-all\n",
    "            else:\n",
    "                node[key] = val\n",
    "        # compile address\n",
    "        if len(address) > 0:\n",
    "            node['address'] = {}\n",
    "            street_full = None\n",
    "            street_dict = {}\n",
    "            street_format = ['prefix', 'name', 'type']\n",
    "            # parse through address objects\n",
    "            for key in address:\n",
    "                val = address[key]\n",
    "                if street_regex.search(key):\n",
    "                    if key == 'street':\n",
    "                        street_full = val\n",
    "                    elif 'street:' in key:\n",
    "                        street_dict[key.replace('street:', '')] = val\n",
    "                else:\n",
    "                    node['address'][key] = val\n",
    "            # assign street_full or fallback to compile street dict\n",
    "            if street_full:\n",
    "                node['address']['street'] = street_full\n",
    "            elif len(street_dict) > 0:\n",
    "                node['address']['street'] = ' '.join([street_dict[key] for key in street_format])\n",
    "        return node\n",
    "    else:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def process_map(file_in, pretty = False):\n",
    "    file_out = \"{0}.json\".format(file_in)\n",
    "    data = []\n",
    "    with codecs.open(file_out, \"w\") as fo:\n",
    "        for _, element in ET.iterparse(file_in):\n",
    "            el = shape_element(element)\n",
    "            if el:\n",
    "                data.append(el)\n",
    "                if pretty:\n",
    "                    fo.write(json.dumps(el, indent=2)+\"\\n\")\n",
    "                else:\n",
    "                    fo.write(json.dumps(el) + \"\\n\")\n",
    "    return data\n",
    "\n",
    "db.doc.insert_many(process_map(data));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### File sizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The original OSM file is 3.329866 MB\n"
     ]
    }
   ],
   "source": [
    "print 'The original OSM file is ' + str(os.path.getsize(data)/1.0e6) + ' MB'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The JSON file is 3.795993 MB\n"
     ]
    }
   ],
   "source": [
    "print 'The JSON file is ' + str(os.path.getsize(data + '.json')/1.0e6) + ' MB'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of documents is 17053\n"
     ]
    }
   ],
   "source": [
    "# Number of documents, we defined it for next section.\n",
    "number_document = db.doc.find().count()\n",
    "print 'The number of documents is ' + str(number_document)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of node is 13332\n"
     ]
    }
   ],
   "source": [
    "# Number of nodes\n",
    "print 'The number of node is ' + str(db.doc.find({'type':'node'}).count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of way is 3721\n"
     ]
    }
   ],
   "source": [
    "# Number of ways\n",
    "print 'The number of way is ' + str(db.doc.find({'type':'way'}).count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of unique users is 91\n"
     ]
    }
   ],
   "source": [
    "# Number of unique users, we defined it for next section.\n",
    "number_unique_users = len(db.doc.distinct('created.user'))\n",
    "print 'The number of unique users is ' + str(number_unique_users)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The first contributor is TELCOM IP with 4921 contributions.\n"
     ]
    }
   ],
   "source": [
    "# Top 1 contributing user\n",
    "cursor = db.doc.aggregate([{'$group':{'_id':'$created.user', 'count':{'$sum':1}}}, {'$sort':{'count':-1}}, {'$limit':1}])\n",
    "for res in cursor:\n",
    "    user1=res['_id']\n",
    "    user1_count=res['count']\n",
    "print 'The first contributor is ' + user1 + ' with '+ str(user1_count) + ' contributions.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There is 22 users appearing only once.\n"
     ]
    }
   ],
   "source": [
    "# Number of users appearing only once (having 1 post), we defined it for next section.\n",
    "user_once=db.doc.aggregate([{'$group':{'_id':'$created.user', 'count':{'$sum':1}}}, \n",
    "                       {'$sort':{'count':1}},\n",
    "                       {'$match':{'count':1}},\n",
    "                       {'$group':{'_id':'null','total':{'$sum':'$count'}}}\n",
    "                        ])\n",
    "for res in user_once:\n",
    "    number_user_once=res['total']\n",
    "\n",
    "print 'There is ' + str(number_user_once) + ' users appearing only once.'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Additional Ideas\n",
    "### Contributor statistics and gamification suggestion\n",
    "The contributions of users seems incredibly skewed, possibly due to automated versus manual map editing (the word “bot” appears in some usernames). Here are some user percentage statistics:  \n",
    "1) Top1 Contributor is TELCOM IP, contribution percentage is 28%.  \n",
    "2) Combined top 2 users are [u'TELCOM IP', u'negro'], contribution percentage is 42%.  \n",
    "3) Combined top 10 users are [u'TELCOM IP', u'negro', u'Diego Sanguinetti', u'pizza4days', u'ovruni', u'WorstFixer',   u'greecemapper', u'Paper_', u'MintCondition', u'dbusse'], contribution percentage is 88%.  \n",
    "4) 24% of users contribute with one post.    \n",
    "5) Thinking about these user percentages from this graph below, I’m reminded of “gamification” as a motivating force for contribution. In the context of the OpenStreetMap, if user data were more prominently displayed, perhaps others would take an initiative in submitting more edits to the map. And, if everyone sees that only 10 of power users are creating more than 88% a of given map, that might spur the creation of more efficient bots, especially if certain gamification elements were present, such as rewards, badges, or a leaderboard."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Calculate the percentage of contribution and find the tup N suers.\n",
    "client = MongoClient()\n",
    "db=client.project\n",
    "number_document = db.doc.find().count()\n",
    "\n",
    "def topn_contrib(n, user=False):\n",
    "    if user==True:\n",
    "        topuser=db.doc.aggregate([{'$group':{'_id':'$created.user', 'count':{'$sum':1}}}, \n",
    "                                 {'$sort':{'count':-1}}, {'$limit':n}\n",
    "                                 ])\n",
    "        top_n_users=[]\n",
    "        for res in topuser:\n",
    "            top_n_users.append(res['_id'])\n",
    "\n",
    "    top_n_contrib=db.doc.aggregate([{'$group':{'_id':'$created.user', 'count':{'$sum':1}}}, \n",
    "                         {'$sort':{'count':-1}}, {'$limit':n},\n",
    "                         {'$group':{'_id':'$created.user','total':{'$sum':'$count'}}}\n",
    "                        ])\n",
    "\n",
    "    for res in top_n_contrib:\n",
    "        top_n_contrib_count=res['total']\n",
    "\n",
    "    percent_contrib_topn=(top_n_contrib_count*100)/number_document\n",
    "    \n",
    "    if user==True:\n",
    "        return top_n_users,percent_contrib_topn\n",
    "    else:\n",
    "        return percent_contrib_topn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top1 Contributor is TELCOM IP, contribution percentage is 28%.\n",
      "Combined top 2 users are [u'TELCOM IP', u'negro'], contribution percentage is 42%.\n",
      "Combined top 10 users are [u'TELCOM IP', u'negro', u'Diego Sanguinetti', u'pizza4days', u'ovruni', u'WorstFixer', u'greecemapper', u'Paper_', u'MintCondition', u'dbusse'], contribution percentage is 88%.\n",
      "24% of users contribute with one post.\n"
     ]
    }
   ],
   "source": [
    "top1,top1_percent_contrib=topn_contrib(1,user=True)\n",
    "print 'Top1 Contributor is ' + top1[0] + ', contribution percentage is ' + str(top1_percent_contrib) + '%.'\n",
    "top2,top2_percent_contrib=topn_contrib(2,user=True)\n",
    "print 'Combined top 2 users are ' + str(top2) + ', contribution percentage is ' + str(top2_percent_contrib) + '%.'\n",
    "top10,top10_percent_contrib=topn_contrib(10, user=True)\n",
    "print 'Combined top 10 users are ' + str(top10) + ', contribution percentage is ' + str(top10_percent_contrib) + '%.'\n",
    "percent_user_1post=(number_user_once*100)/number_unique_users\n",
    "print str(percent_user_1post) + '% of users contribute with one post.'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Additional data exploration using MongoDB queries\n",
    "1) Top 10 appearing amenities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{u'count': 22, u'_id': u'bus_station'}, {u'count': 12, u'_id': u'fuel'}, {u'count': 12, u'_id': u'school'}, {u'count': 7, u'_id': u'restaurant'}, {u'count': 5, u'_id': u'hospital'}, {u'count': 5, u'_id': u'place_of_worship'}, {u'count': 4, u'_id': u'marketplace'}, {u'count': 4, u'_id': u'veterinary'}, {u'count': 4, u'_id': u'townhall'}, {u'count': 4, u'_id': u'university'}]\n"
     ]
    }
   ],
   "source": [
    "amenity = db.doc.aggregate([{'$match':{'amenity':{'$exists':1}}},\n",
    "                               {'$group':{'_id':'$amenity', 'count':{'$sum':1}}},\n",
    "                               {'$sort':{'count':-1}},\n",
    "                               {'$limit':10}])\n",
    "\n",
    "print list(amenity)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2) Biggest religion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{u'count': 3, u'_id': u'christian'}]\n"
     ]
    }
   ],
   "source": [
    "biggest_religion = db.doc.aggregate([{'$match':{'amenity':{'$exists':1}, 'amenity':'place_of_worship'}},\n",
    "                    {'$group':{'_id':'$religion', 'count':{'$sum':1}}},\n",
    "                    {'$sort':{'count':-1}}, {'$limit':1}])\n",
    "\n",
    "print list(biggest_religion)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3) Most popular cuisines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{u'count': 4, u'_id': None}]\n"
     ]
    }
   ],
   "source": [
    "popular_cuisines = db.doc.aggregate([{'$match':{'amenity':{'$exists':1}, 'amenity':'restaurant'}}, \n",
    "                    {'$group':{'_id':'$cuisine', 'count':{'$sum':1}}},\n",
    "                    {'$sort':{'count':-1}}, {'$limit':1}])\n",
    "\n",
    "print list(popular_cuisines)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Conclusion\n",
    "1) The map about the city of San Jose is relatively clean so I could retrieve some interesting content. But still the data is not entirely clean.  \n",
    "2) The data contains some mistakes or different references for the same feature. So I had to clean the data programmatically for the street and the postal codes.  \n",
    "3) When we audit the data, it was very clear that although there are minor error caused by human input, the dataset is fairly well-cleaned. Considering there're hundreds of contributors for this map, there is a great numbers of human errors in this project. I'd recommend a srtuctured input form so everyone can input the same data format to reduce this error.  \n",
    "4) We can incentivize users by gamify the contribution process, then we can create a recommendation engine to leverage these data (eg. restaurant recommendation, building, etc).  \n",
    "5) OpenStreetMaps is an open source project, there're still a lot of areas left unexplored as people tend to focus on a certain key areas and left other part outdated. Since each node has a coordinate (lattitude & longtitude), we can resolve this issue by cross-referencing/cross-validating missing data from other database like Google API."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### References\n",
    "1) https://github.com/GuillaumeSalvan/P3-Wrangle-OpenStreetMap-Data  \n",
    "2) https://github.com/lyvinhhung/Udacity-Data-Analyst-Nanodegree/tree/master/p3%20-%20Wrangle%20OpenStreetMap%20Data"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
